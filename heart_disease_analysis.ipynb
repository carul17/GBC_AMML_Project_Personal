{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Classification Project\n",
    "\n",
    "This notebook implements a comprehensive analysis and classification of heart disease data using multiple machine learning approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Configure visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Description\n",
    "\n",
    "The Heart Disease dataset contains various medical attributes that can be used to predict the presence of heart disease. Here are the features:\n",
    "\n",
    "1. **age**: Age in years\n",
    "2. **sex**: Sex (1 = male, 0 = female)\n",
    "3. **cp**: Chest pain type\n",
    "   - Value 0: Typical angina\n",
    "   - Value 1: Atypical angina\n",
    "   - Value 2: Non-anginal pain\n",
    "   - Value 3: Asymptomatic\n",
    "4. **trestbps**: Resting blood pressure (in mm Hg)\n",
    "5. **chol**: Serum cholesterol in mg/dl\n",
    "6. **fbs**: Fasting blood sugar > 120 mg/dl (1 = true; 0 = false)\n",
    "7. **restecg**: Resting electrocardiographic results\n",
    "   - Value 0: Normal\n",
    "   - Value 1: Having ST-T wave abnormality\n",
    "   - Value 2: Showing probable or definite left ventricular hypertrophy\n",
    "8. **thalach**: Maximum heart rate achieved\n",
    "9. **exang**: Exercise induced angina (1 = yes; 0 = no)\n",
    "10. **oldpeak**: ST depression induced by exercise relative to rest\n",
    "11. **slope**: The slope of the peak exercise ST segment\n",
    "12. **ca**: Number of major vessels colored by fluoroscopy (0-3)\n",
    "13. **thal**: Thalassemia\n",
    "   - Value 1: Normal\n",
    "   - Value 2: Fixed defect\n",
    "   - Value 3: Reversible defect\n",
    "14. **target**: Diagnosis of heart disease (0 = absence, 1 = presence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Cleaning\n",
    "\n",
    "Let's examine and clean our dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Initial Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"\\nNumber of duplicate rows:\", df.duplicated().sum())\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nBasic statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial data exploration reveals:\n",
    "- Dataset shape: 1025 rows Ã— 14 columns\n",
    "- All features are numeric (13 integer columns, 1 float column)\n",
    "- No missing values in any columns\n",
    "- 723 duplicate rows identified\n",
    "- Features include both continuous measurements (age, blood pressure, cholesterol) and categorical variables (sex, chest pain type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions and relationships\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Distribution of target variable\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.countplot(data=df, x='target')\n",
    "plt.title('Distribution of Heart Disease')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable visualization shows:\n",
    "- Binary classification problem (0 = no heart disease, 1 = heart disease present)\n",
    "- Nearly balanced dataset with approximately 51.3% positive cases\n",
    "- Distribution plot indicates good representation of both classes, which is beneficial for model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Age and Gender Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Age distribution by target\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(data=df, x='target', y='age')\n",
    "plt.title('Age Distribution by Heart Disease')\n",
    "plt.xlabel('Heart Disease Present')\n",
    "plt.ylabel('Age')\n",
    "\n",
    "# Age distribution by target and gender\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=df, x='target', y='age', hue='sex')\n",
    "plt.title('Age Distribution by Target and Gender')\n",
    "plt.xlabel('Heart Disease Present')\n",
    "plt.ylabel('Age')\n",
    "plt.legend(title='Gender', labels=['Female', 'Male'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The age distribution plots reveal:\n",
    "- Age distribution by target shows different patterns for heart disease presence vs absence\n",
    "- When broken down by gender:\n",
    "  * Males (sex=1) comprise about 69.6% of the dataset\n",
    "  * Females (sex=0) comprise about 30.4% of the dataset\n",
    "  * Age patterns differ between genders, with males showing higher heart disease risk at younger ages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Clinical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clinical features distribution\n",
    "clinical_features = ['cp', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(clinical_features):\n",
    "    sns.boxplot(data=df, x='target', y=feature, ax=axes[idx])\n",
    "    axes[idx].set_title(f'{feature} by Target')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of key clinical measurements:\n",
    "- Blood Pressure (trestbps):\n",
    "  * Most values fall between 120-140 mmHg\n",
    "  * Higher values tend to be associated with heart disease\n",
    "- Cholesterol (chol):\n",
    "  * Shows wide distribution\n",
    "  * Higher levels generally indicate increased heart disease risk\n",
    "- Maximum Heart Rate (thalach):\n",
    "  * Lower maximum heart rates more common in heart disease cases\n",
    "  * Shows negative correlation with heart disease presence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Medical Risk Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze key risk factors\n",
    "risk_factors = ['fbs', 'exang', 'slope', 'ca', 'thal']\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, factor in enumerate(risk_factors):\n",
    "    sns.countplot(data=df, x=factor, hue='target', ax=axes[idx])\n",
    "    axes[idx].set_title(f'{factor} Distribution by Target')\n",
    "    axes[idx].legend(['No Disease', 'Disease'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical risk factors show distinct patterns:\n",
    "- Chest Pain Type (cp):\n",
    "  * Four categories show varying relationships with heart disease\n",
    "  * Asymptomatic chest pain (type 3) most strongly associated with disease\n",
    "- Exercise Induced Angina (exang):\n",
    "  * Binary feature (0 = no, 1 = yes)\n",
    "  * Strong predictor when present\n",
    "- ST Slope (slope):\n",
    "  * Three categories with different risk levels\n",
    "  * Downsloping indicates higher risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature correlations with target\n",
    "correlations = df.corr()['target'].sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "correlations.plot(kind='bar')\n",
    "plt.title('Feature Correlations with Target')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation analysis highlights:\n",
    "- Strongest positive correlations with heart disease:\n",
    "  * ST depression (oldpeak)\n",
    "  * Number of vessels (ca)\n",
    "  * Exercise induced angina (exang)\n",
    "- Strongest negative correlations:\n",
    "  * Maximum heart rate (thalach)\n",
    "  * Chest pain type (cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove outliers using IQR method\n",
    "def remove_outliers(df, columns):\n",
    "    df_clean = df.copy()\n",
    "    for col in columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
    "    return df_clean\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "categorical_features = ['cp', 'thal', 'slope', 'restecg']\n",
    "\n",
    "# Handle outliers in numerical features\n",
    "df_clean = remove_outliers(df, numerical_features)\n",
    "print(f\"Rows after outlier removal: {len(df_clean)} (removed {len(df) - len(df_clean)} rows)\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df_clean.drop('target', axis=1)\n",
    "y = df_clean['target']\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_features)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale only the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "X_train_scaled[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "X_test_scaled[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "print(\"\\nTraining set shape:\", X_train_scaled.shape)\n",
    "print(\"Testing set shape:\", X_test_scaled.shape)\n",
    "print(\"\\nFeatures after one-hot encoding:\", X_train_scaled.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation and Evaluation\n",
    "\n",
    "We'll implement and evaluate multiple classification models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    return model, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "lr_model, lr_accuracy = evaluate_model(lr_model, X_train_scaled, X_test_scaled, y_train, y_test, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model, dt_accuracy = evaluate_model(dt_model, X_train_scaled, X_test_scaled, y_train, y_test, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model, rf_accuracy = evaluate_model(rf_model, X_train_scaled, X_test_scaled, y_train, y_test, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model\n",
    "sgd_model = SGDClassifier(random_state=42)\n",
    "sgd_model, sgd_accuracy = evaluate_model(sgd_model, X_train_scaled, X_test_scaled, y_train, y_test, \"SGD Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model\n",
    "svm_model = SVC(random_state=42)\n",
    "svm_model, svm_accuracy = evaluate_model(svm_model, X_train_scaled, X_test_scaled, y_train, y_test, \"Support Vector Machine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search(model, param_grid, X_train, y_train, model_name):\n",
    "    # Create GridSearchCV object\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    \n",
    "    # Fit the grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\n{model_name} Grid Search Results:\")\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'max_iter': [100, 200, 300]\n",
    "}\n",
    "\n",
    "lr_best = perform_grid_search(LogisticRegression(random_state=42), \n",
    "                             lr_param_grid, \n",
    "                             X_train_scaled, \n",
    "                             y_train, \n",
    "                             \"Logistic Regression\")\n",
    "\n",
    "# Evaluate tuned model\n",
    "evaluate_model(lr_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Tuned Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "dt_best = perform_grid_search(DecisionTreeClassifier(random_state=42),\n",
    "                             dt_param_grid,\n",
    "                             X_train_scaled,\n",
    "                             y_train,\n",
    "                             \"Decision Tree\")\n",
    "\n",
    "# Evaluate tuned model\n",
    "evaluate_model(dt_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Tuned Decision Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "rf_best = perform_grid_search(RandomForestClassifier(random_state=42),\n",
    "                             rf_param_grid,\n",
    "                             X_train_scaled,\n",
    "                             y_train,\n",
    "                             \"Random Forest\")\n",
    "\n",
    "# Evaluate tuned model\n",
    "evaluate_model(rf_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Tuned Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Classifier Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_param_grid = {\n",
    "    'loss': ['hinge', 'log_loss', 'modified_huber'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'optimal', 'adaptive'],\n",
    "    'eta0': [0.01, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "sgd_best = perform_grid_search(SGDClassifier(random_state=42),\n",
    "                              sgd_param_grid,\n",
    "                              X_train_scaled,\n",
    "                              y_train,\n",
    "                              \"SGD Classifier\")\n",
    "\n",
    "# Evaluate tuned model\n",
    "evaluate_model(sgd_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Tuned SGD Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1]\n",
    "}\n",
    "\n",
    "svm_best = perform_grid_search(SVC(random_state=42),\n",
    "                              svm_param_grid,\n",
    "                              X_train_scaled,\n",
    "                              y_train,\n",
    "                              \"Support Vector Machine\")\n",
    "\n",
    "# Evaluate tuned model\n",
    "evaluate_model(svm_best, X_train_scaled, X_test_scaled, y_train, y_test, \"Tuned SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Selection Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_best.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance, x='importance', y='feature')\n",
    "plt.title('Feature Importance from Random Forest')\n",
    "plt.show()\n",
    "\n",
    "# Select top 8 features\n",
    "top_features = feature_importance['feature'].head(8).tolist()\n",
    "X_selected = X[top_features]\n",
    "\n",
    "# Split and scale the selected features\n",
    "X_train_selected, X_test_selected, y_train, y_test = train_test_split(\n",
    "    X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_selected_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_test_selected_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "print(\"\\nSelected features:\", top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Models with Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models with selected features\n",
    "print(\"\\nModel Performance with Selected Features:\")\n",
    "\n",
    "# Logistic Regression\n",
    "evaluate_model(lr_best, X_train_selected_scaled, X_test_selected_scaled, \n",
    "              y_train, y_test, \"Logistic Regression (Selected Features)\")\n",
    "\n",
    "# Decision Tree\n",
    "evaluate_model(dt_best, X_train_selected_scaled, X_test_selected_scaled, \n",
    "              y_train, y_test, \"Decision Tree (Selected Features)\")\n",
    "\n",
    "# Random Forest\n",
    "evaluate_model(rf_best, X_train_selected_scaled, X_test_selected_scaled, \n",
    "              y_train, y_test, \"Random Forest (Selected Features)\")\n",
    "\n",
    "# SGD Classifier\n",
    "evaluate_model(sgd_best, X_train_selected_scaled, X_test_selected_scaled, \n",
    "              y_train, y_test, \"SGD Classifier (Selected Features)\")\n",
    "\n",
    "# SVM\n",
    "evaluate_model(svm_best, X_train_selected_scaled, X_test_selected_scaled, \n",
    "              y_train, y_test, \"SVM (Selected Features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "1. **Model Performance Comparison**:\n",
    "   - Initial vs Tuned models\n",
    "   - Full features vs Selected features\n",
    "   \n",
    "2. **Feature Importance**:\n",
    "   - Most significant features for prediction\n",
    "   - Impact of feature selection on model performance\n",
    "   \n",
    "3. **Best Performing Model**:\n",
    "   - Which model performed best overall\n",
    "   - Trade-offs between different models\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "Based on our comprehensive analysis of the heart disease dataset using multiple machine learning models, we can draw several important conclusions:\n",
    "\n",
    "1. Data Quality and Preprocessing:\n",
    "- The dataset contains 1025 samples with 13 features and no missing values\n",
    "- There is a good balance between positive (51.3%) and negative (48.7%) cases\n",
    "- Features were standardized to ensure optimal model performance\n",
    "\n",
    "2. Model Performance Comparison:\n",
    "- Logistic Regression achieved 79.5% accuracy\n",
    "  * Good balance between precision (0.85) and recall (0.72) for no heart disease\n",
    "  * Slightly better at identifying positive cases (recall 0.87)\n",
    "\n",
    "- Decision Tree after hyperparameter tuning achieved 98.5% accuracy\n",
    "  * Exceptional performance with precision and recall both around 0.99\n",
    "  * Best performing model among all tested classifiers\n",
    "  * Optimal parameters: max_depth=10, min_samples_leaf=1, min_samples_split=2\n",
    "\n",
    "- Random Forest showed strong performance\n",
    "  * High accuracy and good generalization\n",
    "  * Less prone to overfitting compared to single decision trees\n",
    "\n",
    "- SGD Classifier achieved 75.6% accuracy\n",
    "  * Balanced performance across classes\n",
    "  * Precision and recall both around 0.76\n",
    "\n",
    "3. Feature Selection Impact:\n",
    "- Feature selection helped identify the most important predictors:\n",
    "  * Maximum heart rate (thalach)\n",
    "  * ST depression induced by exercise (oldpeak)\n",
    "  * Number of major vessels colored by fluoroscopy (ca)\n",
    "  * Chest pain type (cp)\n",
    "\n",
    "4. Clinical Implications:\n",
    "- The models show strong potential for supporting medical diagnosis\n",
    "- Decision Tree and Random Forest models are particularly promising for clinical use\n",
    "- Feature importance analysis provides insights into key diagnostic indicators\n",
    "- High accuracy across multiple models suggests reliable predictive capability\n",
    "\n",
    "5. Limitations and Future Work:\n",
    "- Consider collecting more data to improve model robustness\n",
    "- Explore ensemble methods combining multiple models\n",
    "- Validate findings with external datasets\n",
    "- Investigate potential biases in the dataset\n",
    "- Consider implementing real-time prediction system for clinical use\n",
    "\n",
    "6. Model Selection Recommendations:\n",
    "- For highest accuracy: Use the tuned Decision Tree model\n",
    "- For balanced performance: Consider Logistic Regression\n",
    "- For interpretability: Decision Tree provides clear decision rules\n",
    "- For robust predictions: Random Forest offers good generalization\n",
    "\n",
    "This analysis demonstrates the potential of machine learning in supporting heart disease diagnosis while highlighting the importance of model selection and feature engineering. The high performance across multiple models suggests strong predictive capability for heart disease diagnosis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
